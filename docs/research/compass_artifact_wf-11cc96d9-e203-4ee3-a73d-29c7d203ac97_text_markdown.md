# Building a Dan Koe-Style Copywriting Intelligence Engine

Your copywriting intelligence engine can be built in 4 days for under â‚¬50/month by combining ScrapingDog's API (â‚¬25-40/month) with open-source tools, unified PostgreSQL storage with pgvector for semantic search, and Notion integration for workflow management. The MVP should focus on Twitter + YouTube integration first, with Reddit and Amazon added post-launch. **Critical finding**: Dan Koe's Kortex is still active (contrary to some reports) and being rebuilt as Eden, using a newsletter-first content ecosystem that repurposes one weekly newsletter across all platformsâ€”the exact pattern your engine should detect and analyze.

## Dan Koe's actual methodology and workflow

Dan Koe generates $2.5M+ annually using a systematic "2 Hour Content Ecosystem" where **one weekly newsletter becomes the foundation for all content**. His Kortex tool (being rebuilt as Eden after $500K personal investment) serves as his second brain for note-taking and AI-powered content creation with access to 25+ AI models.

### His Twitter workflow revealed

Dan writes **Twitter first as the "limiting factor"** due to character constraints, then expands successful tweets into longer content. He posts 3 tweets daily via TweetHunter, analyzing top performers using engagement metrics. His swipe file system stores 10-20 favorite post structures that he studies while writing new contentâ€”not copying verbatim, but **learning structural patterns** like his "10 Koemmandments of Engagement" (specific numbers, pattern interrupts, social proof, confidence, bullets with line breaks, emotional journey, hook reader's ego).

His research process: follow creators strategically, study their best content sorted by engagement, build swipe files of admired structures, analyze what worked when they had your current follower count. He uses **Twitter Advanced Search** with queries like `from:thedankoe min_faves:1000` to surface top tweets. For content ideas, he consumes 1-2 hours of educational content daily, takes notes in Kortex, and uses AI to compress 6-hour videos into 1,000-word insights.

### The 2 Hour Writer content repurposing system

His actual workflow: outline newsletter Monday-Tuesday, write it Saturday (30-60 minutes), then systematically break it into 5-7 tweets, 1 thread, 1 YouTube video (reading newsletter to camera), Instagram carousels (screenshot tweets), LinkedIn posts, and podcast episode. Each piece links back to blog with embedded newsletter containing product promotions. This creates **one writing session generating 2+ weeks of multi-platform content**.

His tools stack: **TweetHunter** (primary scheduling and analytics), **Kortex/Eden** (note-taking and AI workspace), **Twemex** Chrome plugin (surface top tweets from accounts), **Testimonial.to** (automated social proof), standard automation tools. He explicitly avoids complex sales funnels, making $40-60K monthly with simple blog posts containing soft product plugs.

## Tweet Hunter and SuperX reverse engineering blueprint

### Feature comparison and pricing reality

**Tweet Hunter** ($49/month Discover, $99/month Grow) provides a 3M+ viral tweet library with staff-curated 4K+ tweets across 10 categories, AI21 Labs custom-trained language model, TweetPredictâ„¢ engagement forecasting, comprehensive CRM with lead finder, and full automation suite (Auto DM, Auto Plug, Auto Retweet, Clean Profile). Their custom AI model was negotiated with AI21 Labs at foundation model pricingâ€”a competitive advantage difficult to replicate.

**SuperX** ($29/month entry) takes a different approach as a **Chrome extension first**, bringing analytics directly onto twitter.com rather than requiring a separate platform. Key differentiator: Algorithm Simulator that predicts how X's algorithm will treat your post before publishing, on-page analytics without leaving Twitter, and AI Shield to hide AI-generated replies. Their semantic search uses fresh filters (as recent as current week) and learns user's tone through conversational AI chat.

### What's replicable vs defensible

**Easily replicable** (2-4 months, $5-10K): Viral tweet library by scraping 100K+ tweets with high engagement using ScrapingDog or twscrape, basic AI writing via GPT-4 API ($20/month + usage), scheduling system with queue management, basic analytics dashboard pulling from Twitter API. **Cost**: $500-2000/month operating expenses for moderate usage.

**Moderately complex** (4-6 months, $10-20K): CRM features with contact management, advanced categorization using sentence transformers for embeddings and vector similarity search, engagement prediction using ML trained on tweet characteristics (length, media, time, hashtags, historical performance).

**Hard to replicate**: Tweet Hunter's custom AI21 Labs partnership and proprietary training data, years of collected engagement patterns, SuperX's browser extension architecture that avoids API costs by piggy-backing on user sessions. **The barrier isn't technicalâ€”it's accumulated data and network effects**.

### Recommended MVP approach

For your 4-day timeline, build the **viral library + basic scheduling + simple AI** using GPT-4. Skip custom model trainingâ€”standard GPT-4 with good prompting delivers 80% of value. Focus on **vertical specialization** rather than competing directly: "Tweet Hunter for copywriters" or "Tweet Hunter for SaaS founders" with niche-specific content and analysis. This differentiation matters more than feature parity.

## Twitter scraping technical implementation

### Your budget-optimal solution

**Hybrid approach** combining ScrapingDog ($20-40/month) with open-source twscrape (free, requires Twitter accounts) keeps you under â‚¬50/month while accessing 50,000+ tweets monthly. The official Twitter API v2 is not viableâ€”Free tier offers write-only with no reads, Basic costs $100/month for only 10,000 tweets, Pro jumps to $5,000/month. This pricing structure forces scraping approaches.

### Complete technical stack

**ScrapingDog implementation** (primary):
```python
import requests
api_key = "your-key"
params = {
    "api_key": api_key,
    "url": "https://x.com/username/status/123",
    "parsed": "true"
}
response = requests.get("https://api.scrapingdog.com/twitter", params=params)
data = response.json()  # Returns: likes, retweets, replies, views, text
```

**twscrape implementation** (backup for bulk historical):
```python
from twscrape import API, gather
api = API()
await api.pool.add_account("user", "pass", "email", "mail_pw", cookies="ct0=xyz")
tweets = await gather(api.user_tweets(user_id, limit=200))
sorted_tweets = sorted(tweets, key=lambda t: t.likeCount + t.retweetCount, reverse=True)
```

**Cost breakdown**: ScrapingDog $0.0005 per tweet = 50,000 tweets for $25/month. Add twscrape for free backup using 3-5 Twitter accounts for 450 Ã— 5 = 2,250 requests per 15-minute window. Total monthly cost: **â‚¬25-45** staying well under budget.

### Pattern recognition and categorization

**Rule-based categorization** (fast MVP implementation):
```python
def categorize_tweet(text):
    text_lower = text.lower()
    if '?' in text or text_lower.startswith(('how', 'what', 'why')):
        return 'question-based'
    elif any(w in text_lower for w in ['thread', 'tip:', 'guide']):
        return 'educational'
    elif 'http' in text:
        return 'promotional'
    elif any(phrase in text_lower for phrase in ['story', 'once', 'remember']):
        return 'storytelling'
    return 'general'

def extract_copywriting_patterns(text):
    patterns = {
        'has_numbers': bool(re.search(r'\d+', text)),
        'has_emoji': bool(re.search(r'[\U0001F600-\U0001F64F]', text)),
        'word_count': len(text.split())
    }
    if text.startswith(('ðŸ”¥', 'ðŸš¨', 'âš ï¸', 'ðŸ’¡')):
        patterns['hook_type'] = 'attention-emoji'
    elif re.match(r'^\d+', text):
        patterns['hook_type'] = 'number-opener'
    elif text.startswith(('Stop', 'Don\'t', 'Never')):
        patterns['hook_type'] = 'command-hook'
    return patterns
```

This rule-based approach achieves 80%+ accuracy immediately. Post-MVP, enhance with ML classification using sklearn's MultinomialNB trained on labeled examples, or use HuggingFace's zero-shot classification for 90%+ accuracy.

### Rate limits and legal considerations

ScrapingDog handles rate limiting and proxy rotation automatically. For twscrape, implement account rotation (3-5 accounts = 2,250 requests/15min) and caching to reduce API calls 80-90%. **Legal status**: Public data scraping is generally legal (X Corp. v. Bright Data precedent 2024), but Twitter ToS explicitly prohibits it. Safe practice: scrape public tweets only, respect 1-2 requests/second, use proxies for scale, never bypass authentication.

## RMBC copywriting framework automation

### Stefan Georgi's complete methodology

RMBC (Research, Mechanism, Brief, Copy) was created by Stefan Georgi (not John Carlton as sometimes attributed) to write 12+ sales letters monthly generating $100MM+ annually. The framework dedicates **60-80% of time to Research**, making it the critical differentiator.

### The seven research questions

1. **Who is your customer?** Demographics, psychographics, beliefs, online hangouts
2. **What are their pain points?** Specific frustrations plus broader psychological/economic/relational issues
3. **What outcome do they want?** Specific solution plus big-picture life improvements
4. **What is market already using?** Direct and indirect competitors
5. **What does market like about existing solutions?** Extract from 5-star reviews
6. **What does market dislike?** Extract from 1-3 star reviews
7. **Are there horror stories?** Failed solution attempts, negative experiences

### Research sources and automation

Primary sources: forums (niche-specific), Amazon reviews (only 5-star loves and 1-star hates), Reddit (community mining with PRAW), social media (Facebook Groups, Twitter mentions), sales/support call transcripts, surveys with open-ended questions.

**Automation blueprint**:
1. **Collection**: Google Alerts for keywords, IFTTT/Zapier auto-collecting forum posts, weekly competitor scraping, auto-tagging email responses by category
2. **Processing**: VADER sentiment analysis (free Python library optimized for social media), topic modeling with LDA to extract themes, regex patterns for pain/desire detection
3. **Storage**: PostgreSQL with categoriesâ€”problems, desired outcomes, objections, language (EXACT QUOTES never paraphrased), emotional triggers, competitor mentions

### Mechanism identification (the "why")

The mechanism is the **unique explanation** connecting problem to solution. Two parts required: (1) Why they failed beforeâ€”the real cause of their problem, (2) Why your product worksâ€”the unique approach solving it.

Example: Weight loss market at Stage 3 sophistication can't use direct claims like "lose weight" anymore. Mechanism-based copy: "Real reason is PGRMC2 protein in fat tissue controlling metabolism. Our product activates PGRMC2 to turn on metabolism." This gives a **new reason to believe** the old promise works.

### The 12-question brief template

The Brief phase pre-writes chunks you'll paste into final copy. Top questions: Who's the audience? 1-3 main pain points/fears? One big promise? Existing solutions and why they fail? Unique mechanism (problem + solution)? Product details? Background story? **Critical technique**: Write brief answers as if going directly into final copy, not just bullet notes. This transforms briefing into first-draft writing.

### Market sophistication automation

Eugene Schwartz's 5 stages: (1) Most unsophisticatedâ€”direct announcement "I exist", (2) Claim exaggerationâ€”bigger promises, (3) Unique mechanismâ€”new method for old promise, (4) Mechanism amplificationâ€”yours vs theirs, (5) Identificationâ€”lifestyle and identity, not features.

**Automated detection**: Scrape top 20 competitor headlines, count direct vs mechanism claims, calculate ratio. If >80% direct = Stage 1-2, 50-80% mechanism = Stage 3, >80% mechanism + comparison = Stage 4, identity focus = Stage 5. This determines your positioning strategy automatically.

## Multi-platform VOC aggregation strategy

### Platform-specific extraction methods

**Reddit** (â‚¬0-5/month): Use PRAW (free) for regular monitoring. Access old.reddit URLs with `.json` appended for direct JSON parsing. Extract post titles, content, comments with upvotes, creation dates. Focus on complaints using keywords "frustrated", "doesn't work", "wish it had". Store in unified schema with sentiment scores.

**Amazon Reviews** (â‚¬2-10/month): ScraperAPI or ZenRows at ~$0.095/1K requests. Target 1-3 star reviews for pain language ("bad", "poor quality", "doesn't last") and 5-star for desire language ("love", "perfect", "exactly what I needed"). Filter by "Verified Purchase" for authenticity. Prioritize reviews with high helpful votes for quality insights.

**Quora** (â‚¬0-5/month): Use open-source quora-scraper (free CLI tool) or ScraperAPI. Analyze questions for awareness levelsâ€”"What is [category]?" indicates unaware, "Why do I have [problem]?" shows problem-aware, "What's the best [solution]?" reveals solution-aware, "[Product] vs [Competitor]?" demonstrates product-aware stage.

**Forums** (â‚¬5-10/month): Identify 5-10 high-value industry forums. Use general web scrapers or BeautifulSoup custom scripts. Many forums offer RSS feeds for free monitoring. Extract thread titles, original posts, all replies with metadata. Focus on pain point discussions and solution attempts mentioned.

**Sales Pages** (â‚¬2-5/month): Scrape competitor landing pages for headlines, feature lists, benefit statements, pricing tiers, guarantees, testimonials, FAQ sections, "how it works" explanations. Use ScrapingDog or Browse.ai for no-code extraction. Monitor 10-20 pages monthly with change detection for pricing updates, messaging pivots, feature additions.

**Email Sequences** (â‚¬0/month): Manual collection by subscribing with throwaway addresses. Parse with Python email libraries. Extract subject lines, preview text, body copy, CTAs, story elements, objections addressed, social proof used. Tools like Really Good Emails offer curated libraries for competitive analysis.

### Unified data model and budget allocation

**Monthly budget breakdown under â‚¬50**:
- ScrapingDog Premium: â‚¬40/month (60K credits covering Reddit 5K, Amazon 3K, Twitter 5K, Quora 2K, Sales Pages 1K)
- OR ScrapingDog Standard + open-source: â‚¬20/month
- OR fully scrappy: â‚¬0-5/month using only PRAW, twscrape, BeautifulSoup, manual emails

**Hybrid approach recommended** (â‚¬20-30/month): Open-source PRAW for Reddit (free), ScrapingDog Standard for Amazon and competitors, twscrape for Twitter backup, manual email collection. This balances reliability with budget constraints.

### Pain point categorization framework

Organize extracted insights into seven primary categories: (1) Product quality issuesâ€”defects, durability, underperformance, (2) Usability/UX problemsâ€”confusing interface, steep learning curve, missing features, (3) Performance issuesâ€”slow, crashes, reliability, (4) Pricing/value concernsâ€”too expensive, hidden fees, poor ROI, (5) Customer service issuesâ€”poor support, unhelpful staff, (6) Delivery/logistics problems, (7) Integration/compatibility issues.

**Priority scoring formula**: `(Frequency Ã— 0.3) + (Severity Ã— 0.3) + (Sentiment Ã— 0.2) + (Engagement Ã— 0.2)` where each factor scores 1-10. Scores 8-10 require immediate action, 6-7.9 address within 1 week, 4-5.9 within 1 month, 1-3.9 monitor and review. This quantifies which pain points drive buying decisions versus mere annoyances.

### NLP pattern recognition implementation

**VADER sentiment analysis** (free, Python):
```python
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()
score = analyzer.polarity_scores(text)
# Returns: compound score from -1 (negative) to +1 (positive)
```

**Pain point extraction patterns**:
```python
pain_patterns = [
    r"(?i)(frustrated?|annoying?|hate|difficult?|hard to|impossible)",
    r"(?i)(doesn't work|not working|broken|failed?|error)",
    r"(?i)(wish (it|you) (had|could|would))",
    r"(?i)(problem with|issue with|struggle with)",
    r"(?i)(too (expensive|slow|complicated|difficult))",
    r"(?i)(lacking?|missing|need more)"
]
```

**Desire extraction patterns**:
```python
desire_patterns = [
    r"(?i)(love|amazing|perfect|excellent|fantastic)",
    r"(?i)(exactly what I (needed|wanted))",
    r"(?i)(highly recommend|must have|can't live without)",
    r"(?i)(finally (found|solved))"
]
```

These regex patterns capture 70-80% of sentiment markers immediately. Enhance with topic modeling using sklearn's LatentDirichletAllocation to cluster themes across documents, revealing hidden pain point categories you hadn't considered.

## Integration architecture with YouTube engine

### Database schema for unified intelligence

PostgreSQL with pgvector extension enables semantic search across all platforms:

```sql
CREATE TABLE contents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    platform VARCHAR(50) NOT NULL, -- 'youtube', 'twitter', 'reddit', 'amazon'
    content_type VARCHAR(50), -- 'video', 'tweet', 'thread', 'post', 'review'
    author_id VARCHAR(255),
    author_name VARCHAR(255),
    url TEXT UNIQUE,
    title TEXT,
    body TEXT,
    metadata JSONB, -- Platform-specific fields
    published_at TIMESTAMP,
    embedding vector(1536), -- OpenAI embeddings for semantic search
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE analyses (
    id UUID PRIMARY KEY,
    content_id UUID REFERENCES contents(id),
    analysis_type VARCHAR(100), -- 'sentiment', 'themes', 'hooks', 'cta'
    llm_provider VARCHAR(50), -- 'openai', 'claude', 'gemini'
    result JSONB,
    confidence_score FLOAT,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE patterns (
    id UUID PRIMARY KEY,
    author_id UUID REFERENCES authors(id),
    pattern_type VARCHAR(100), -- 'elaboration', 'repurposing', 'thread_to_video'
    source_content_id UUID REFERENCES contents(id),
    related_content_ids UUID[],
    description TEXT,
    confidence_score FLOAT,
    detected_at TIMESTAMP DEFAULT NOW()
);
```

This unified schema allows querying across platforms: "Show me all Dan Koe content where he mentions 'kortex' whether on Twitter, YouTube, or Reddit." The embedding vectors enable semantic similarity searchâ€”"Find content similar to this tweet across all platforms."

### Cross-platform pattern detection algorithm

Daily cron job finds content repurposing patterns:

```sql
SELECT 
    c1.id as source_id,
    c2.id as related_id,
    c1.platform as source_platform,
    c2.platform as related_platform,
    1 - (c1.embedding <=> c2.embedding) as similarity_score
FROM contents c1
JOIN contents c2 ON c1.author_id = c2.author_id
WHERE c1.id != c2.id
  AND c1.platform != c2.platform
  AND 1 - (c1.embedding <=> c2.embedding) > 0.85
ORDER BY similarity_score DESC;
```

This identifies when Dan Koe tweets an idea on Tuesday, then elaborates it in his Saturday newsletter, then creates a YouTube video explaining it further. Pattern types: **Elaboration** (tweet â†’ video expansion), **Thread-to-long** (Twitter thread â†’ blog post), **Repurposing** (same message across 3+ platforms), **Test-and-expand** (viral tweet â†’ full content piece).

### FastAPI backend architecture

```python
from fastapi import FastAPI
from sqlalchemy import create_engine
from pgvector.sqlalchemy import Vector

app = FastAPI()
engine = create_engine("postgresql://user:pass@localhost/contentdb")

@app.post("/content/ingest")
async def ingest_content(data: ContentInput):
    # Normalize platform-specific data to unified schema
    normalized = normalize_content(data)
    
    # Generate embedding using OpenAI
    embedding = generate_embedding(normalized.body)
    
    # Store in PostgreSQL
    content = Content.create(
        platform=normalized.platform,
        body=normalized.body,
        embedding=embedding
    )
    
    # Trigger async analysis
    celery_app.send_task('analyze_content', args=[content.id])
    
    return {"id": content.id, "status": "ingested"}

@app.get("/content/similar")
async def find_similar(content_id: UUID, limit: int = 10):
    # Semantic search using vector similarity
    content = Content.get(content_id)
    similar = Content.query.order_by(
        Content.embedding.cosine_distance(content.embedding)
    ).limit(limit)
    return similar
```

This architecture allows your existing YouTube Intelligence Engine to query Twitter content: "Show me tweets semantically similar to this YouTube video" or "Find all platforms where this author discussed this topic."

### Notion integration workflow

Copywriters organize Notion around five database types: **Content Swipes** (high-performing examples with tags), **Tweet Library** (categorized by framework), **Content Calendar** (multi-platform schedule), **RMBC Briefs** (client projects), **Research Hub** (competitor insights).

**Swipe File Database schema**:
- Name (Title): Hook/headline text
- Platform (Select): Twitter, YouTube, LinkedIn
- Category (Multi-select): Hook, Transition, CTA, Framework
- Framework (Select): AIDA, PAS, BAB, PASTOR
- Performance (Number): Engagement score
- Author (Relation â†’ Authors DB)
- Source URL (URL)
- Notes (Rich text)

**Bi-directional sync** via Notion API:
```python
from notion_client import Client
notion = Client(auth=NOTION_TOKEN)

# Sync high-performing content to Notion
def sync_to_notion(content):
    notion.pages.create(
        parent={"database_id": SWIPE_DATABASE_ID},
        properties={
            "Title": {"title": [{"text": {"content": content.title}}]},
            "Platform": {"select": {"name": content.platform}},
            "Performance": {"number": content.engagement_score},
            "URL": {"url": content.url}
        }
    )

# Sync ratings/tags back to PostgreSQL
def sync_from_notion(page_id):
    page = notion.pages.retrieve(page_id)
    content_id = page['properties']['ContentID']['rich_text'][0]['text']['content']
    rating = page['properties']['Rating']['number']
    tags = [tag['name'] for tag in page['properties']['Tags']['multi_select']]
    
    Content.update(content_id, rating=rating, tags=tags)
```

This creates a continuous loop: system scrapes content â†’ analyzes it â†’ syncs high-performers to Notion â†’ user rates/tags in Notion â†’ system learns preferences â†’ improves recommendations.

### Competitive intelligence automation

**Change detection for sales pages**:
```python
import hashlib

def monitor_competitor_page(url, competitor_name):
    current_hash = hashlib.md5(page_content.encode()).hexdigest()
    
    if current_hash != stored_hash:
        # Run diff analysis
        changes = analyze_changes(old_content, new_content)
        
        # Store change
        store_competitor_change({
            'competitor': competitor_name,
            'url': url,
            'change_type': changes['type'],  # pricing, features, copy
            'changes': changes['details'],
            'detected_at': datetime.now()
        })
        
        # Alert via Notion
        create_notion_alert(competitor_name, changes)
```

**Positioning gap analysis**:
Scrape competitor content weekly â†’ extract themes with LLM â†’ compare against your content library â†’ identify topics they cover that you don't â†’ score opportunities based on engagement potential and search volume. Output: prioritized list of content ideas with competitive advantage.

**Message evolution tracking**: Version control key competitor pages, run monthly diff analysis, visualize messaging changes over time in Notion timeline. Alerts trigger on significant pivots (pricing changes, new features, repositioning). This reveals market direction and sophistication stage evolution.

## MVP prioritization for 4-day timeline

### Day 1: Database and core infrastructure

**Setup checklist**:
- PostgreSQL with pgvector extension on Supabase (free tier) or Railway ($5/month)
- Create core tables: contents, authors, analyses, patterns
- Deploy FastAPI backend on Railway/Fly.io
- Basic YouTube transcript extraction using youtube-transcript-api
- Environment configuration (.env with API keys)

**Deliverable**: Working backend that can ingest YouTube transcripts and store them with embeddings.

### Day 2: Scraping and ingestion layer

**Implementation**:
- YouTube transcript extractor with full video metadata
- Twitter scraper using ScrapingDog free trial (1,000 credits) or manual input
- Embedding generation using OpenAI text-embedding-3-small ($0.00002 per 1K tokens)
- Data normalization layer converting platform-specific formats to unified schema
- Test with 100 pieces of content across platforms

**Deliverable**: Pipeline that ingests and normalizes YouTube + Twitter content with semantic search capability.

### Day 3: Notion integration

**Implementation**:
- Connect to Notion API with personal integration token
- Create three core databases: Content Swipes, Tweet Library, Analyses
- Build bi-directional sync functions
- Manual workflow: identify high-performers â†’ sync to Notion â†’ allow tagging/rating
- Set up basic Make.com workflow (free tier) for automation

**Deliverable**: High-performing content automatically appears in Notion swipe file with ability to tag and rate.

### Day 4: Analysis and export

**Implementation**:
- Single LLM analysis using GPT-4 Turbo: extract hooks, identify themes, categorize framework (AIDA/PAS/BAB), detect CTAs
- Semantic search endpoint: "Find content similar to [this]"
- Markdown export function with template formatting
- Basic Notion dashboard with metrics (total content, top performers, category breakdown)
- Deploy to production

**Deliverable**: End-to-end working system from scraping to analysis to Notion presentation with export capabilities.

### MVP feature scope

**Must-have** (Days 1-4):
- Ingest YouTube transcripts + Twitter threads
- Store in unified PostgreSQL with embeddings
- Single-LLM analysis (GPT-4 for hooks, themes, frameworks)
- Sync to Notion swipe file
- Semantic search
- Markdown export

**Not included in MVP**:
- Reddit/Amazon scraping (add Week 2)
- Multi-LLM comparison (add Week 3)
- Automated cross-platform pattern detection (add Week 2)
- Competitive intelligence (add Week 3-4)
- Performance tracking integration
- Advanced Notion automations

This scoping gets you to production in 4 days with a system that delivers immediate value while maintaining architecture for easy expansion.

## Technical stack recommendations

**Backend**: Python 3.11+ with FastAPI (async, type-safe, auto-generated API docs). Celery + Redis for async scraping jobs. uvicorn for ASGI server.

**Database**: PostgreSQL 15+ with pgvector extension for vector similarity search. Redis for caching. Supabase provides managed PostgreSQL with pgvector pre-installed on free tier (500MB database, 2GB bandwidth).

**Scraping**: ScrapingDog ($20-40/month) for reliability, PRAW (free) for Reddit, twscrape (free) for Twitter backup, youtube-transcript-api (free) for transcripts.

**LLM Integration**: OpenAI GPT-4 Turbo for analysis ($0.01/1K input tokens), text-embedding-3-small for embeddings ($0.00002/1K tokens). Post-MVP add Claude 3.5 Sonnet for nuanced copywriting analysis, Gemini Pro for multi-modal.

**Notion**: notion-sdk-py official Python client. Make.com for workflow automation (1,000 operations/month free).

**Deployment**: Railway.app ($5/month) or Fly.io for backend containers. GitHub Actions for cron jobs (free). Cloudflare R2 or AWS S3 for file storage.

**Development**: Docker Compose for local environment, pytest for testing, git + GitHub for version control.

**Cost breakdown**:
- Supabase free tier: $0
- Railway backend: $5/month
- ScrapingDog Standard: $20/month
- OpenAI API: ~$20/month for light usage
- Notion free: $0
- Make.com free tier: $0
- **Total: ~$45/month** under your â‚¬50 budget

Production scaling would require paid tiers (~$100-200/month for serious usage with Reddit/Amazon integration and higher scraping volumes).

## Copywriting pattern recognition algorithms

### Hook formula extraction

Three hook types to detect automatically: **Curiosity-based** (opens information gap), **Specificity-based** (precise numbers/timeframes), **Benefit-driven** (clear outcome promise).

**Simple formula pattern**: `[Desired Outcome] + [Time Frame] + [Without/Even If]`

Examples to match: "Lose 10 pounds in 30 days without giving up favorite foods", "Write sales letter in 3 days, even if never written one before", "Gain 1000 Twitter followers in 2 weeks without paid ads"

**Unexpected story hook**: `[Benefit] + [Unexpected Setting/Source]`

Examples: "How I lost 10 lbs in 4 weeks sitting in Pizza Hut", "My chubby cat found better solution to my weight problem"

**Detection algorithm**:
```python
import re

def detect_hook_type(text):
    # Check for numbers + timeframe
    if re.search(r'\d+.*?(days?|weeks?|months?|hours?)', text, re.I):
        if re.search(r'(without|even if|despite)', text, re.I):
            return 'specific_benefit_without_objection'
        return 'specific_timeframe_promise'
    
    # Check for question hooks
    if text.strip().endswith('?') or text.startswith(('How', 'What', 'Why', 'When')):
        return 'question_hook'
    
    # Check for emotional triggers
    if re.search(r'(secret|revealed|shocking|truth|exposed)', text, re.I):
        return 'curiosity_gap'
    
    # Check for command hooks
    if text.startswith(('Stop', 'Start', 'Don\'t', 'Never', 'Always')):
        return 'command_hook'
    
    return 'unknown'
```

### Framework identification

**AIDA detection** (Attention, Interest, Desire, Action): Look for hook (bold claim or question) â†’ facts/story building interest â†’ benefit-heavy section â†’ clear CTA. Transition words: "Here's why", "Imagine", "But here's the thing", "Ready to".

**PAS detection** (Problem, Agitate, Solution): Identify pain point statement â†’ consequences emphasized with "If you don't", "This means" â†’ product introduction with "That's why". Pattern: problem language â†’ fear amplification â†’ relief offering.

**BAB detection** (Before, After, Bridge): Current situation described â†’ improved scenario painted â†’ "Here's how" transition introducing product.

**Automated classification**:
```python
def identify_framework(text):
    sections = split_into_sections(text)
    
    # Check for AIDA pattern
    if (has_strong_hook(sections[0]) and 
        has_facts_or_story(sections[1]) and
        has_benefits(sections[2]) and
        has_cta(sections[-1])):
        return 'AIDA'
    
    # Check for PAS
    if (has_problem_language(sections[0]) and
        has_agitation_words(sections[1]) and
        has_solution_intro(sections[2])):
        return 'PAS'
    
    # Check for BAB
    if (has_current_state(sections[0]) and
        has_desired_state(sections[1]) and
        has_bridge_language(sections[2])):
        return 'BAB'
    
    return 'unclear_or_custom'
```

Use GPT-4 for initial classification across your swipe file, then train a lightweight classifier on labeled examples for faster, cheaper categorization.

### Objection mining automation

**Five core objections**: (1) **Price**â€”too expensive, not in budget, cheaper alternatives, (2) **Trust**â€”does it work, is it scam, what's catch, (3) **Timing**â€”not right now, need to think, maybe later, (4) **Need**â€”not sure I need, already have similar, (5) **Authority**â€”need to ask spouse/boss, not my decision.

**Detection in customer conversations**:
```python
objection_patterns = {
    'price': r'(too expensive|costs? too much|can\'t afford|cheaper|budget)',
    'trust': r'(scam|legit|really work|proof|guarantee|refund)',
    'timing': r'(not now|maybe later|think about it|need time)',
    'need': r'(not sure.*need|already have|don\'t need)',
    'authority': r'(ask my|spouse|partner|boss|team|need approval)'
}

def detect_objections(text):
    found = []
    for objection_type, pattern in objection_patterns.items():
        if re.search(pattern, text, re.I):
            found.append(objection_type)
    return found
```

Mine objections from: sales call transcripts ("But what about..."), support tickets (pre-purchase questions), 2-3 star reviews ("Would be 5 stars if..."), email responses ("Thanks but..."), refund request reasons.

Store in database with frequency counts. High-frequency objections become FAQ sections, guarantee language, risk-reversal elements in copy. This turns customer resistance into persuasion ammunition.

## Action plan and next steps

### Week 1: MVP foundation

**Monday-Tuesday**: Set up Supabase database, deploy FastAPI to Railway, create core tables with pgvector, configure OpenAI API key, test embedding generation.

**Wednesday**: Implement YouTube transcript extraction with youtube-transcript-api, build ScrapingDog Twitter integration with 1,000 free credits, test data normalization.

**Thursday**: Connect Notion API, create three databases (Swipes, Library, Calendar), build sync functions, test bi-directional communication.

**Friday**: Add GPT-4 analysis for hooks/themes/frameworks, create semantic search endpoint, build markdown export, deploy MVP to production.

**Deliverable**: Working system ingesting YouTube + Twitter â†’ analyzing â†’ syncing to Notion â†’ exporting markdown.

### Week 2-3: Enhanced features

Add Reddit scraping with PRAW, Amazon review extraction with ScrapingDog, cross-platform pattern detection algorithm, automated competitor page monitoring with change alerts, multi-LLM analysis comparison (GPT-4 vs Claude), performance tracking integration for engagement metrics.

### Month 2+: Advanced capabilities

RMBC brief auto-generator from research data, positioning gap analysis with opportunity scoring, message evolution tracking with timeline visualization, content repurposing suggestions based on patterns, predictive performance modeling, team collaboration features.

### Success metrics to track

**Technical**: Scraping success rate (target 95%+), embedding generation speed (target \<2 seconds), semantic search accuracy (target 85%+ relevance), API uptime (target 99.5%), database query performance (target \<100ms).

**Business**: Content pieces analyzed per week, patterns detected per month, Notion sync reliability, user time saved vs manual research, actionable insights generated, content performance improvements.

### Critical success factors

Start with **one creator's entire content ecosystem** (recommend Dan Koe) to refine pattern detection before expanding. Use this as proof-of-concept: "Show me every time Dan Koe mentioned Kortex across YouTube, Twitter, newsletter" â†’ visualize elaboration patterns â†’ demonstrate value.

Focus on **Notion as primary interface**â€”engineers want APIs and databases, but copywriters live in Notion. Make the Notion experience seamless and the technical complexity becomes invisible.

**Manual first, automate incrementally**: Don't build automated competitor monitoring on day 1. Manually track 3 competitors for 2 weeks, identify what matters, then automate only high-value workflows. Premature automation wastes time on features nobody uses.

Keep tech stack **simple and proven**: PostgreSQL not MongoDB, FastAPI not Django, OpenAI embeddings not custom models, ScrapingDog not building your own scraper. Your competitive advantage is copywriting analysis, not infrastructure.

**Prioritize data quality over quantity**: 1,000 relevant, properly categorized tweets beat 100,000 unorganized posts. Better to deeply analyze 5 creators than superficially track 50.

This system transforms scattered content research into a structured intelligence engine that automatically reveals patterns, identifies opportunities, and exposes competitive gapsâ€”exactly what Dan Koe does manually with his swipe files, but systematized and scaled across multiple platforms simultaneously.