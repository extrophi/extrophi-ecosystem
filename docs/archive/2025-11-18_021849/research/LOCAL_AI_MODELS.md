# Local AI Model Deployment Research
## On-Device AI Model Architecture & Performance Optimization

**Date**: November 16, 2025
**Status**: Research Complete - Recommendations Ready for Implementation
**Target Application**: BrainDump v3.0 (Voice Journaling Desktop App)

---

## Executive Summary

This research evaluates on-device AI model deployment strategies for the BrainDump voice journaling application. Key findings:

1. **Whisper Models**: Latest Whisper Large V3 Turbo offers 8x faster inference than Large V2 with equivalent accuracy
2. **Quantization**: INT4/Q4_0 quantization reduces model size by 70-80% with minimal accuracy loss
3. **Local LLMs**: Phi-3 Mini (3.8B) is optimal for memory-constrained devices; Mistral 7B for higher quality
4. **Model Serving**: llama.cpp with OpenAI-compatible API provides seamless integration path
5. **Performance**: Apple Silicon optimization via Metal Performance Shaders (MPS) provides 3-5x speedup

**Recommendation**: Implement modular architecture with:
- Whisper Base model (78M parameters) for transcription
- Phi-3 Mini (3.8B) for local chat responses
- Llama.cpp server for OpenAI-compatible inference
- MLX framework for Apple Silicon optimization

---

## 1. Whisper Model Variants & Selection

### Model Comparison Matrix

| Model | Parameters | Accuracy | Speed | File Size (GGML) | VRAM (GPU) | CPU RAM | Best For |
|-------|------------|----------|-------|------------------|------------|---------|----------|
| **Tiny** | 39M | ~95% (WER) | 2000 FPS | 75 MB | 1 GB | 2 GB | Real-time, low power |
| **Tiny.en** | 39M | ~97% (WER) | 2000 FPS | 75 MB | 1 GB | 2 GB | English-only, fastest |
| **Base** | 74M | ~97% (WER) | 1100 FPS | 141 MB | 1.5 GB | 3 GB | Standard choice |
| **Base.en** | 74M | ~98% (WER) | 1100 FPS | 141 MB | 1.5 GB | 3 GB | English-only, better |
| **Small** | 244M | ~98% (WER) | 400 FPS | 461 MB | 3 GB | 6 GB | Balanced quality |
| **Small.en** | 244M | ~98.5% (WER) | 400 FPS | 461 MB | 3 GB | 6 GB | English-only, higher accuracy |
| **Medium** | 769M | ~98.5% (WER) | 130 FPS | 1.5 GB | 6 GB | 12 GB | High accuracy |
| **Large-v3** | 1.5B | ~99.1% (WER) | 40 FPS | 2.9 GB | 8 GB | 16 GB | Highest accuracy, production |
| **Large-v3 Turbo** | 1.5B | ~99.1% (WER) | 320 FPS | 2.9 GB | 8 GB | 16 GB | **NEW: 8x faster than Large!** |

### Key Recommendations

#### For BrainDump (Recommended Configuration)
**Primary Choice: Whisper Base or Small**
- **Why**: Base provides 97-98% WER accuracy, runs on GPU in 1.5GB VRAM
- **Model**: `ggml-base.bin` (currently in codebase)
- **Implementation**: Already integrated via whisper.cpp FFI

#### Alternative: Distil-Whisper Large-V3
- **Specs**: Reduced from 32 decoder layers â†’ 4 decoder layers
- **Performance**: Equivalent accuracy to Large-V2, 50% faster
- **File Size**: Same as Large but ~2x throughput
- **Availability**: Available on Hugging Face in GGML format
- **Command**:
  ```bash
  model_name="distil-whisper/distil-large-v3-ggml"
  # Convert and quantize: ./quantize models/ggml-distil-large-v3.bin q5_0
  ```

#### For Edge/Mobile: Whisper Tiny
- **Constraint**: Low-power devices, Raspberry Pi
- **Trade-off**: 5% accuracy loss vs Base
- **Benefit**: Runs on 1GB VRAM, 2GB system RAM
- **Use Case**: Secondary device support if expanded

### Whisper Model Quantization

**INT4 (Q4_0) Quantization**:
```
Original Model Size: 141 MB (base.bin FP32)
After Q4_0: ~35 MB (75% reduction)
Accuracy Impact: <2% WER degradation
Speed Impact: +10-20% faster inference
```

**Recommended Quantization Levels**:
- Q4_0: Highest compatibility, best speed/quality
- Q5_0: Slight quality improvement over Q4_0
- Q6_0: Minimal quality gain, not recommended
- Q8_0: Negligible improvement, larger file size

**Build GGML Model**:
```bash
# Clone whisper.cpp
git clone https://github.com/ggml-org/whisper.cpp.git
cd whisper.cpp
make

# Download and convert model
./models/download-ggml-model.sh base

# Optional: Quantize to Q5_0
./quantize models/ggml-base.bin models/ggml-base-q5_0.bin q5_0
```

---

## 2. Alternative Speech Recognition Models

### Model Comparison

| Model | Type | Size | Speed | Accuracy | Offline | License | Notes |
|-------|------|------|-------|----------|---------|---------|-------|
| **Whisper** | Transformer | 39M-1.5B | 40-2000 FPS | 95-99.1% | âœ“ | MIT | Best overall; multilingual |
| **Distil-Whisper** | Distilled | 1.5B | 300+ FPS | 99% | âœ“ | MIT | Fast Whisper variant |
| **Vosk** | Kaldi DNN-HMM | ~50M | Real-time | 85-92% | âœ“ | Apache 2.0 | Lightweight, edge devices |
| **wav2vec 2.0** | Self-supervised | 300M-1B | 100-500 FPS | 95%+ | âœ“ | MIT | Facebook's model, language-specific |
| **Conformer** | Transformer | 120M-300M | 100-400 FPS | 96-98% | âœ“ | Various | Better accent handling |
| **Massively Multilingual** | Transformer | 1B-2B | 50-150 FPS | 90-95% (100+ langs) | âœ“ | Various | Supports 100+ languages |

### Evaluation for BrainDump

#### âœ… Whisper (Current Choice - OPTIMAL)
**Advantages**:
- Already integrated via whisper.cpp
- Best accuracy-to-speed ratio
- Multilingual support (if needed for future expansion)
- Active community, well-documented
- GGML optimization available

**Disadvantages**:
- Largest base model (1.5B for Large)
- Requires more VRAM than Vosk

#### ğŸ”„ Alternative: Vosk (Lightweight Option)
**Advantages**:
- Extremely lightweight (~50MB models)
- Real-time on CPU
- Works on Raspberry Pi, mobile devices
- Offline, privacy-first

**Disadvantages**:
- Lower accuracy (85-92% vs 98%+ for Whisper)
- Limited model quality
- Less active development
- Not ideal for transcription quality

**When to Consider**: If expanding to ultra-lightweight deployment (IoT, mobile) or resource-constrained environments.

#### ğŸ”¬ wav2vec 2.0 Integration Path
**Advantages**:
- Self-supervised pre-training
- Excellent accuracy
- Available through Hugging Face Transformers

**Disadvantages**:
- Requires PyTorch/Transformers (adds dependencies)
- Slower than whisper.cpp for real-time use
- Language-specific models

**Implementation Note**: Can be alternative if Whisper FFI becomes problematic, but current Whisper.cpp FFI is superior.

#### Conformer Models
**Best Use Case**: If fine-tuning on custom accents needed in future
- Available through NeMo framework
- Better multi-accent support
- Higher training flexibility

---

## 3. Local LLM Options for Chat Responses

### Model Selection Matrix

| Model | Parameters | Type | VRAM (FP16) | VRAM (INT4) | CPU RAM | Context | Speed (tokens/sec) |
|-------|------------|------|------------|------------|---------|---------|-------------------|
| **Phi-3 Mini** | 3.8B | Instruction-tuned | 7.1 GB | 1.8 GB | 8 GB | 128K | 15-25 |
| **Phi-3 Medium** | 14B | Instruction-tuned | 26 GB | 6.5 GB | 32 GB | 128K | 8-12 |
| **TinyLlama** | 1.1B | General | 2.2 GB | 0.6 GB | 4 GB | 4K | 40-60 |
| **Mistral 7B** | 7B | Instruction-tuned | 13.7 GB | 3.4 GB | 16 GB | 32K | 10-15 |
| **Mistral 8x7B (Mixtral)** | 56B (4 active) | MoE | 46 GB | ~8-12 GB | 64 GB | 32K | 5-10 |
| **Llama 2 7B** | 7B | General | 13.7 GB | 3.4 GB | 16 GB | 4K | 10-15 |
| **Llama 3 8B** | 8B | Instruction-tuned | 15.7 GB | 3.9 GB | 16 GB | 8K | 8-12 |

### BrainDump Recommendation: Phi-3 Mini

**Why Phi-3 Mini**:
- âœ… Smallest full-featured instruction model (3.8B vs 7B+)
- âœ… Designed for edge deployment on consumer hardware
- âœ… INT4 quantized: only 1.8GB VRAM (runs on M1/M2 Macs)
- âœ… 128K context window (enough for conversation history)
- âœ… Microsoft officially supports it for on-device deployment
- âœ… Excellent quality for journaling use case

**Hardware Requirements**:
```
Minimum (INT4 Quantized):
- 8GB system RAM
- 2GB VRAM (GPU) or use CPU
- M1/M2 Mac with 8GB unified memory

Recommended (FP16):
- 16GB system RAM
- 8GB VRAM (RTX 3060 or better)
- M2 Pro/Max Macs with 16GB+ unified memory
```

**Performance Expectations**:
- INT4 (quantized): 12-18 tokens/second
- FP16 (full precision): 15-25 tokens/second
- Context processing: ~2-3 seconds for 8K token context

### Secondary Option: Mistral 7B

**Advantages**:
- Higher quality responses than Phi-3 Mini
- Better instruction-following
- Proven reliability in production

**Disadvantages**:
- 7B parameters = 2x memory requirement
- Overkill for journaling use case
- Slower response times

**When to Choose**: If users demand higher response quality and have 16GB+ RAM.

### Lightweight Alternative: TinyLlama

**Advantages**:
- Only 1.1B parameters
- 2.2GB VRAM (FP16) or 600MB INT4
- Still coherent responses

**Disadvantages**:
- Lower quality than Phi-3 Mini
- No instruction tuning
- Limited context window (4K)

**Use Case**: Educational deployments, extreme resource constraints.

---

## 4. Model Serving Architecture

### Recommended: llama.cpp Server + OpenAI-Compatible API

#### Why llama.cpp
- **C/C++ implementation**: Standalone executable, no Python/Rust required
- **OpenAI API compatibility**: Drop-in replacement for Claude/OpenAI
- **Streaming support**: Real-time token streaming for responsive UI
- **Native Metal support**: Automatic Apple Silicon optimization
- **Cross-platform**: CPU, CUDA, Metal, ROCm support

#### Architecture Diagram
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             BRAINDUMP FRONTEND                      â”‚
â”‚          (Tauri + Svelte 5)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ HTTP (Chat messages)
                     â”‚ Streaming SSE (responses)
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LLAMA.CPP SERVER (Port 8080)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  OpenAI-Compatible API                       â”‚  â”‚
â”‚  â”‚  /v1/chat/completions (streaming)            â”‚  â”‚
â”‚  â”‚  /v1/models (list loaded models)             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â”‚                                  â”‚
             â–¼                                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Phi-3 Mini  â”‚              â”‚ Metal GPU        â”‚
        â”‚ (3.8B GGUF) â”‚              â”‚ Accelerator      â”‚
        â”‚             â”‚              â”‚ (Apple Silicon)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Setup Instructions

**1. Download llama.cpp**:
```bash
git clone https://github.com/ggml-org/llama.cpp.git
cd llama.cpp

# Build with Metal support (Apple Silicon)
make LLAMA_METAL=1

# Or with CUDA (NVIDIA GPU)
make LLAMA_CUDA=1
```

**2. Download Phi-3 Mini Model (GGUF format)**:
```bash
# Using huggingface-cli
pip install huggingface-hub[cli]

# Download Phi-3-mini-4k-instruct in GGUF format
huggingface-cli download \
  microsoft/Phi-3-mini-4k-instruct-gguf \
  Phi-3-mini-4k-instruct-q4.gguf \
  --local-dir ./models

# Or download from Ollama repository
curl -L https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf \
  -o models/phi-3-mini-q4.gguf
```

**3. Start Server**:
```bash
# Basic startup
./llama-server \
  -m ./models/phi-3-mini-q4.gguf \
  --port 8080 \
  -n 1024 \
  --gpu-layers 35

# For Apple Silicon with Metal acceleration
./llama-server \
  -m ./models/phi-3-mini-q4.gguf \
  --port 8080 \
  -n 1024 \
  --gpu-layers 35 \
  -ngl 99 \
  -t 4

# With context length configuration
./llama-server \
  -m ./models/phi-3-mini-q4.gguf \
  --port 8080 \
  -n 512 \
  --ctx-size 4096 \
  --cache-type-k f16 \
  -ngl 35
```

**4. Integration in Tauri Commands** (`src-tauri/src/commands.rs`):

```rust
#[tauri::command]
pub async fn send_local_llm_message(
    message: String,
    session_id: i64,
    state: tauri::State<'_, AppState>
) -> Result<String, String> {
    let client = reqwest::Client::new();

    // Get conversation history
    let messages = state.db.get_messages(session_id)
        .map_err(|e| e.to_string())?
        .iter()
        .map(|msg| {
            serde_json::json!({
                "role": if msg.sender == "user" { "user" } else { "assistant" },
                "content": msg.content
            })
        })
        .collect::<Vec<_>>();

    // Call local llama.cpp server
    let response = client
        .post("http://localhost:8080/v1/chat/completions")
        .json(&serde_json::json!({
            "model": "local",
            "messages": messages,
            "max_tokens": 512,
            "temperature": 0.7,
            "stream": false
        }))
        .send()
        .await
        .map_err(|e| e.to_string())?
        .text()
        .await
        .map_err(|e| e.to_string())?;

    let json: serde_json::Value = serde_json::from_str(&response)
        .map_err(|e| e.to_string())?;

    let content = json["choices"][0]["message"]["content"]
        .as_str()
        .unwrap_or("")
        .to_string();

    // Save response to database
    state.db.create_message(&Message {
        session_id,
        sender: "assistant".to_string(),
        content: content.clone(),
        created_at: chrono::Utc::now(),
        recording_id: None,
    })
    .map_err(|e| e.to_string())?;

    Ok(content)
}
```

#### Testing with curl
```bash
# Health check
curl http://localhost:8080/v1/models

# Single message
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "local",
    "messages": [
      {"role": "user", "content": "What is the capital of France?"}
    ],
    "max_tokens": 100
  }'

# With streaming
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "local",
    "messages": [
      {"role": "user", "content": "Tell me a story"}
    ],
    "stream": true
  }'
```

### Alternative: Python-Based Serving

**llama-cpp-python**:
```bash
pip install llama-cpp-python[server]

# Start with Phi-3
MODEL=./models/phi-3-mini-q4.gguf python3 -m llama_cpp.server \
  --port 8000 \
  --n_gpu_layers 35
```

**Advantages**:
- Pure Python, easier to integrate with Python dependencies
- Same OpenAI-compatible API
- Better streaming support

**Disadvantages**:
- Requires Python environment
- Slightly slower than C++ llama.cpp
- More dependencies

---

## 5. Performance Optimization

### Apple Silicon Optimization (Metal Performance Shaders)

#### Understanding Apple Silicon Advantage
- **Unified Memory**: GPU and CPU share memory (no PCIe bottleneck)
- **Metal Performance Shaders**: Hardware-accelerated compute operations
- **Memory Bandwidth**: 100+ GB/s (vs CPU ~50 GB/s)
- **Power Efficiency**: 3-5x faster inference than CPU

#### llama.cpp Metal Configuration

**Default Configuration**:
```bash
./llama-server \
  -m ./models/phi-3-mini-q4.gguf \
  --gpu-layers 35  # Offload 35 layers to GPU
```

**Tuning Parameters**:
```bash
# Maximum GPU offloading (all layers)
-ngl 99

# With larger context window
--ctx-size 8192

# For faster token generation (lower quality)
-t 4  # 4 threads

# For accuracy (more threads)
-t 8  # 8 threads
```

**Performance Expectations**:
- **M1/M1 Pro**: 12-18 tokens/sec (Phi-3 Mini)
- **M2/M2 Pro**: 15-25 tokens/sec
- **M3/M3 Pro**: 20-35 tokens/sec
- **M4 Max**: 40-60 tokens/sec

#### MLX Alternative (Apple-Optimized)
```bash
pip install mlx-lm

# Run directly
python -m mlx_lm.generate \
  --model microsoft/Phi-3-mini-4k-instruct \
  --max-tokens 100
```

**MLX Advantages**:
- 100% Apple-native framework
- Zero external dependencies
- WWDC 2024 optimizations (Scaled Dot Product Attention)
- Competitive performance with llama.cpp

**Performance Comparison**:
- llama.cpp: Faster token generation (15-20% better)
- MLX: Better memory efficiency, lower latency startup

**Recommendation**: Use llama.cpp for production (better performance), evaluate MLX for future Apple-first optimization.

### NVIDIA GPU Optimization (CUDA)

**Build with CUDA Support**:
```bash
make LLAMA_CUDA=1 CUDA_PATH=/usr/local/cuda
```

**Configuration**:
```bash
./llama-server \
  -m ./models/mistral-7b-q4.gguf \
  --gpu-layers 50 \
  -c 4096
```

**VRAM Requirements**:
- RTX 3060 (12GB): Phi-3 Mini (full), Mistral 7B (quantized)
- RTX 3090 (24GB): Mistral 7B (full precision)
- A100 (80GB): Large models with full context

### CPU Inference Optimization

**When GPU Unavailable**:
```bash
./llama-server \
  -m ./models/phi-3-mini-q4.gguf \
  -t 8 \
  --cpu-threads 8 \
  -c 2048
```

**Performance Expectations**:
- Phi-3 Mini (INT4): 3-5 tokens/sec
- Phi-3 Mini (FP16): 1-2 tokens/sec
- Mistral 7B (INT4): 1-2 tokens/sec

**Optimization Tips**:
- Use DDR5 RAM if possible (50+ GB/s bandwidth)
- Ensure adequate thread count (match CPU cores)
- Reduce context window for faster inference
- Use quantized models (INT4 essential for CPU)

### Quantization Strategy

**Quantization Levels for Phi-3 Mini**:

| Format | File Size | VRAM (GPU) | CPU RAM | Quality | Recommended For |
|--------|-----------|------------|---------|---------|-----------------|
| **FP32** | 14 GB | 15 GB | 28 GB | Perfect | High-end servers |
| **FP16** | 7 GB | 7.5 GB | 14 GB | Perfect | Workstations, Pro Macs |
| **Q5_K** | 3.5 GB | 4 GB | 7 GB | Excellent | Consumer GPUs |
| **Q4_K** | 2.5 GB | 3 GB | 5 GB | Excellent | **BrainDump Default** |
| **Q4_0** | 2.5 GB | 3 GB | 5 GB | Very Good | Edge devices |
| **Q3_K** | 1.8 GB | 2.2 GB | 3.5 GB | Good | Mobile, IoT |
| **IQ3_M** | 1.3 GB | 1.5 GB | 2.5 GB | Acceptable | Ultra-lightweight |

**Recommendation**: Q4_K for production (best balance of quality and file size).

---

## 6. Deployment Architecture for BrainDump

### Complete System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BRAINDUMP APP                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Frontend: Svelte 5                                         â”‚
â”‚  â”œâ”€ Recording Interface (cpal)                              â”‚
â”‚  â”œâ”€ Chat Panel (messages, input)                            â”‚
â”‚  â””â”€ Settings Panel (model selection)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tauri Backend                                              â”‚
â”‚  â”œâ”€ Audio Recording: cpal + hound                           â”‚
â”‚  â”‚  â””â”€ Recording â†’ WAV (16kHz)                              â”‚
â”‚  â”‚                                                           â”‚
â”‚  â”œâ”€ Transcription: whisper.cpp FFI                          â”‚
â”‚  â”‚  â”œâ”€ Model: ggml-base.bin (optional: Q5_0)                â”‚
â”‚  â”‚  â””â”€ Output: Transcript text                              â”‚
â”‚  â”‚                                                           â”‚
â”‚  â”œâ”€ Chat Management: SQLite (local storage)                 â”‚
â”‚  â”‚  â”œâ”€ Sessions (title, created_at)                         â”‚
â”‚  â”‚  â”œâ”€ Messages (role, content, timestamp)                  â”‚
â”‚  â”‚  â””â”€ Recordings (path, duration)                          â”‚
â”‚  â”‚                                                           â”‚
â”‚  â””â”€ AI Response: Two-Path System                            â”‚
â”‚     â”œâ”€ Path A: Local llama.cpp server                       â”‚
â”‚     â”‚  â”œâ”€ Model: phi-3-mini-q4.gguf                         â”‚
â”‚     â”‚  â”œâ”€ Server: http://localhost:8080                     â”‚
â”‚     â”‚  â”œâ”€ Advantage: Privacy, no API cost                   â”‚
â”‚     â”‚  â””â”€ Fallback: If server not running                   â”‚
â”‚     â”‚                                                        â”‚
â”‚     â””â”€ Path B: Cloud API (Claude/OpenAI)                    â”‚
â”‚        â”œâ”€ Model: User-selected (OpenAI/Claude)              â”‚
â”‚        â”œâ”€ Advantage: Better quality, latest features        â”‚
â”‚        â””â”€ Requires: API key, internet connection            â”‚
â”‚                                                              â”‚
â”‚  â”œâ”€ Privacy Scanner (client-side)                           â”‚
â”‚  â”‚  â””â”€ Regex-based PII detection before sending             â”‚
â”‚  â”‚                                                           â”‚
â”‚  â””â”€ Database: Repository pattern                            â”‚
â”‚     â””â”€ All operations through Repository<T>                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Phases

#### Phase 1: Enable Local LLM Option (Current)
**What**: Add UI toggle for "Local Mode" in Settings
**Components**:
- Settings UI: Radio button for "Local" vs "Cloud"
- Tauri command: `send_local_llm_message()`
- Config: llama.cpp port (default 8080)

**Time**: 4-6 hours

#### Phase 2: Auto-Start Local Server (Enhancement)
**What**: Automatically start llama.cpp server on app launch
**Implementation**:
```rust
// In main.rs
fn start_local_llm_server() -> Result<Child> {
    let server_path = tauri::utils::platform::resource_dir()?
        .join("bin/llama-server");

    let child = Command::new(server_path)
        .arg("-m")
        .arg(model_path)
        .arg("--port")
        .arg("8080")
        .spawn()?;

    Ok(child)
}
```

**Time**: 6-8 hours

#### Phase 3: Model Selection UI (Enhancement)
**What**: Allow users to download/select different models
**Features**:
- Model browser (Hugging Face integration)
- Download progress indicator
- Model size warnings
- Storage location management

**Time**: 12-16 hours

#### Phase 4: Context Window Management (Polish)
**What**: Optimize conversation history for context limits
**Features**:
- Summarization of old messages
- Context window indicator
- Token counting
- Conversation trimming

**Time**: 8-10 hours

---

## 7. Production Deployment Checklist

### Pre-Release Requirements

- [ ] **Whisper Model**
  - [x] ggml-base.bin included in app bundle
  - [ ] Model versioning system
  - [ ] Update mechanism for new versions

- [ ] **Local LLM Server**
  - [ ] llama-server binary included in app bundle
  - [ ] Auto-start/auto-stop on app lifecycle
  - [ ] Error handling for server crashes
  - [ ] Port conflict resolution

- [ ] **Model Files**
  - [ ] Phi-3 Mini pre-downloaded in installer
  - [ ] Alternative models available via UI
  - [ ] Storage location: `~/Library/Application Support/com.braindump.app/models/`
  - [ ] Disk space warnings

- [ ] **Performance**
  - [ ] Tested on M1, M2, M3, M4 Macs
  - [ ] Tested on Intel Macs with eGPU
  - [ ] CPU-only fallback mode
  - [ ] Memory usage monitoring

- [ ] **Privacy & Security**
  - [ ] No data sent to remote servers (local mode)
  - [ ] API keys stored in Keychain (cloud mode)
  - [ ] Model files not exposed in sandbox

- [ ] **Documentation**
  - [ ] Model installation guide
  - [ ] Performance tuning guide
  - [ ] Troubleshooting guide
  - [ ] API selection guide

### Runtime Monitoring

```rust
// Health check command
#[tauri::command]
pub fn get_ai_system_status() -> AISystemStatus {
    AISystemStatus {
        whisper_available: check_whisper_model(),
        local_llm_available: check_llama_server(),
        selected_provider: get_selected_provider(),
        models: {
            whisper: get_whisper_model_info(),
            llm: get_llm_model_info(),
        },
        system_resources: {
            available_ram: get_available_ram(),
            available_vram: get_available_vram(),
        },
    }
}
```

---

## 8. Implementation Code Examples

### Tauri Command: Send Local Message with Streaming

```rust
// src-tauri/src/commands.rs

use tauri::State;
use reqwest::Client;
use futures_util::StreamExt;

#[tauri::command]
pub async fn send_local_llm_message_streaming(
    message: String,
    session_id: i64,
    state: State<'_, AppState>,
    window: tauri::Window,
) -> Result<String, String> {
    // Get conversation history
    let messages = state
        .db
        .get_messages(session_id)
        .map_err(|e| e.to_string())?
        .into_iter()
        .rev() // Reverse chronological order
        .take(10) // Last 10 messages for context
        .rev()
        .map(|msg| {
            serde_json::json!({
                "role": if msg.sender == "user" { "user" } else { "assistant" },
                "content": msg.content
            })
        })
        .collect::<Vec<_>>();

    let mut messages = messages;
    messages.push(serde_json::json!({
        "role": "user",
        "content": message.clone()
    }));

    let client = Client::new();

    let mut response = client
        .post("http://localhost:8080/v1/chat/completions")
        .json(&serde_json::json!({
            "model": "local",
            "messages": messages,
            "max_tokens": 1024,
            "temperature": 0.7,
            "stream": true
        }))
        .send()
        .await
        .map_err(|e| format!("Failed to connect to local LLM: {}", e))?;

    if !response.status().is_success() {
        return Err(format!(
            "LLM Server error: {} - {}",
            response.status(),
            response.text().await.unwrap_or_default()
        ));
    }

    let mut full_response = String::new();
    let mut stream = response.bytes_stream();

    while let Some(chunk) = stream.next().await {
        let chunk = chunk.map_err(|e| e.to_string())?;
        let chunk_str = String::from_utf8_lossy(&chunk);

        for line in chunk_str.lines() {
            if line.starts_with("data: ") {
                let data = &line[6..];
                if data == "[DONE]" {
                    break;
                }

                if let Ok(json) = serde_json::from_str::<serde_json::Value>(data) {
                    if let Some(content) = json["choices"][0]["delta"]["content"].as_str() {
                        full_response.push_str(content);

                        // Emit event for real-time UI update
                        window
                            .emit("chat_response_chunk", content)
                            .ok();
                    }
                }
            }
        }
    }

    // Save complete response to database
    state
        .db
        .create_message(&Message {
            session_id,
            sender: "assistant".to_string(),
            content: full_response.clone(),
            created_at: chrono::Utc::now(),
            recording_id: None,
        })
        .map_err(|e| e.to_string())?;

    Ok(full_response)
}
```

### Frontend: Streaming Response Display

```svelte
<!-- src/components/ChatPanel.svelte -->
<script>
    import { invoke } from '@tauri-apps/api/core';
    import { listen } from '@tauri-apps/api/event';

    let { messages = $bindable([]), sessionId } = $props();
    let currentResponse = $state('');
    let isLoading = $state(false);

    let unlistenChunk;

    $effect.pre(() => {
        unlistenChunk?.();
    });

    async function sendMessage(text) {
        isLoading = true;
        currentResponse = '';
        messages = [...messages, { role: 'user', content: text }];

        // Listen for streaming chunks
        unlistenChunk = await listen('chat_response_chunk', (event) => {
            currentResponse += event.payload;
        });

        try {
            const result = await invoke(
                'send_local_llm_message_streaming',
                { message: text, sessionId }
            );

            messages = [...messages, { role: 'assistant', content: result }];
            currentResponse = '';
        } catch (error) {
            messages = [...messages, {
                role: 'error',
                content: `Error: ${error}`
            }];
        } finally {
            isLoading = false;
        }
    }
</script>

<div class="chat-container">
    {#each messages as msg (msg.id)}
        <div class="message {msg.role}">
            {@html marked(msg.content)}
        </div>
    {/each}

    {#if currentResponse}
        <div class="message assistant streaming">
            {@html marked(currentResponse)}
            <span class="cursor">â–Œ</span>
        </div>
    {/if}

    <input
        on:keydown={(e) => e.key === 'Enter' && sendMessage(e.target.value)}
        disabled={isLoading}
        placeholder="Type a message..."
    />
</div>

<style>
    .message.streaming .cursor {
        animation: blink 0.7s infinite;
    }

    @keyframes blink {
        0%, 49% { opacity: 1; }
        50%, 100% { opacity: 0; }
    }
</style>
```

---

## 9. Troubleshooting Guide

### Issue: "Cannot connect to local LLM server"

**Solution 1**: Start llama.cpp server manually
```bash
cd /path/to/llama.cpp
./llama-server -m ./models/phi-3-mini-q4.gguf --port 8080
```

**Solution 2**: Check if port 8080 is in use
```bash
lsof -i :8080  # See what's using port 8080
# If something else, change llama-server port:
./llama-server -m models/phi-3-mini-q4.gguf --port 8081
```

**Solution 3**: Enable auto-start in app settings
```rust
// In Settings panel, toggle "Auto-start local LLM server"
// This should start the server on app launch
```

### Issue: "Out of memory" errors

**Solution 1**: Reduce context window
```bash
./llama-server -m model.gguf --ctx-size 2048  # Default: 4096
```

**Solution 2**: Use more aggressive quantization
```bash
# Switch from Q5_K to Q4_0
./llama-server -m phi-3-mini-q4_0.gguf --port 8080
```

**Solution 3**: Enable CPU offloading (if using GPU)
```bash
# Offload only essential layers
./llama-server -m model.gguf --gpu-layers 20  # Reduce from default 35
```

### Issue: Slow token generation (< 5 tokens/sec)

**Diagnosis**: Check if GPU is being used
```bash
# Should show GPU usage
./llama-server -m model.gguf -ngl 99  # Force GPU offloading

# Monitor GPU in Activity Monitor (macOS)
# or nvidia-smi (Linux/Windows with NVIDIA)
```

**Solution 1**: Ensure GPU offloading is enabled
```bash
./llama-server -m model.gguf -ngl 99 --gpu-layers 99
```

**Solution 2**: Reduce batch size
```bash
./llama-server -m model.gguf --batch-size 128
```

**Solution 3**: Use smaller context window
```bash
./llama-server -m model.gguf --ctx-size 2048
```

### Issue: Whisper transcription accuracy degradation

**Symptoms**: Transcripts have many errors or garbled words

**Solution 1**: Check audio quality
- Ensure microphone is working
- Test with external audio device
- Check noise levels

**Solution 2**: Use larger Whisper model
```bash
# Switch from base to small
./quantize models/ggml-small.bin models/ggml-small-q5_0.bin q5_0
```

**Solution 3**: Ensure whisper.cpp is up to date
```bash
cd whisper.cpp
git pull origin master
make clean
make
```

---

## 10. Future Roadmap

### Q1 2025: Core Implementation
- [ ] Integrate llama.cpp server launcher
- [ ] Add Phi-3 Mini model to app bundle
- [ ] Implement "Local Mode" toggle in Settings
- [ ] Add streaming response display
- [ ] Test on M1, M2, M3 Macs

### Q2 2025: Enhanced Features
- [ ] Model download manager UI
- [ ] Context window management
- [ ] Auto-conversation summarization
- [ ] Performance profiling UI
- [ ] Multi-model switching

### Q3 2025: Advanced Optimization
- [ ] MLX framework evaluation for M4 Macs
- [ ] INT2 quantization support
- [ ] LoRA fine-tuning for custom responses
- [ ] GPU memory optimization
- [ ] Streaming audio output (TTS)

### Q4 2025: Production Release
- [ ] Full test coverage for AI components
- [ ] Performance benchmarks for all Mac models
- [ ] Comprehensive documentation
- [ ] User onboarding for local LLM setup
- [ ] Version 1.0 release with both cloud and local AI options

---

## 11. References & Resources

### Official Documentation
- **Whisper.cpp**: https://github.com/ggml-org/whisper.cpp
- **Llama.cpp**: https://github.com/ggml-org/llama.cpp
- **Phi-3**: https://github.com/microsoft/phi-3
- **Metal Performance Shaders**: https://developer.apple.com/metal/

### Model Downloads
- **Hugging Face**: https://huggingface.co
- **GGUF Models**: https://huggingface.co/TheBloke
- **Microsoft Phi Models**: https://huggingface.co/microsoft

### Community Resources
- **MLX Documentation**: https://ml-explore.github.io/mlx/
- **Ollama**: https://ollama.ai/ (Alternative: simpler model management)
- **LM Studio**: https://lmstudio.ai/ (GUI-based local LLM)

### Performance Benchmarks
- **Hardware Comparison**: https://github.com/ggml-org/llama.cpp/discussions
- **Quantization Analysis**: https://github.com/allisonandreyev/WhisperQuantization
- **MLX Benchmarks**: https://ml-explore.github.io/mlx/benchmarks.html

---

## Appendix A: Model File Locations

### Recommended Directory Structure
```
~/Library/Application Support/com.braindump.app/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ speech/
â”‚   â”‚   â”œâ”€â”€ ggml-base.bin (141 MB - included in bundle)
â”‚   â”‚   â””â”€â”€ ggml-base-q5_0.bin (optional, faster)
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ phi-3-mini-q4.gguf (2.5 GB - auto-downloaded)
â”‚   â”‚   â”œâ”€â”€ phi-3-mini-q5.gguf (3.5 GB - optional alternative)
â”‚   â”‚   â””â”€â”€ mistral-7b-q4.gguf (4 GB - for users who want higher quality)
â”‚   â””â”€â”€ llm-config.json (model metadata)
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ conversation_embeddings/ (future: for RAG)
â”‚   â””â”€â”€ model_cache/
â””â”€â”€ logs/
    â””â”€â”€ llm-server.log
```

### Download URLs
```json
{
  "models": {
    "phi-3-mini-q4": {
      "url": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf",
      "size": "2.5 GB",
      "checksum": "sha256:..."
    },
    "phi-3-mini-fp16": {
      "url": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct.gguf",
      "size": "7 GB",
      "checksum": "sha256:..."
    },
    "mistral-7b-q4": {
      "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
      "size": "4.4 GB",
      "checksum": "sha256:..."
    }
  }
}
```

---

## Appendix B: Configuration Files

### llama.cpp Server Config (config.json)
```json
{
  "port": 8080,
  "host": "127.0.0.1",
  "models": [
    {
      "path": "~/Library/Application Support/com.braindump.app/models/llm/phi-3-mini-q4.gguf",
      "name": "Phi-3 Mini",
      "context_size": 4096,
      "gpu_layers": 35,
      "threads": 4,
      "batch_size": 128
    }
  ],
  "safety": {
    "max_tokens": 1024,
    "temperature": 0.7,
    "top_p": 0.9,
    "repeat_penalty": 1.1
  },
  "performance": {
    "cache_type": "f16",
    "flash_attention": true,
    "mmap": true
  }
}
```

### App Settings (SQLite metadata table)
```sql
INSERT INTO metadata VALUES
('ai_provider', 'local'),  -- 'local' or 'cloud'
('local_llm_model', 'phi-3-mini-q4'),
('local_llm_server', 'http://localhost:8080'),
('whisper_model', 'base'),  -- 'tiny', 'base', 'small', 'medium'
('auto_start_llm_server', 'true'),
('context_window', '4096'),
('max_response_tokens', '512');
```

---

## Summary

**Recommended Configuration for BrainDump v3.0**:

| Component | Selection | Reason |
|-----------|-----------|--------|
| **Speech Recognition** | Whisper Base (GGML) | Current; excellent accuracy; 141 MB |
| **Local LLM** | Phi-3 Mini (Q4 GGUF) | 3.8B params; 1.8 GB; edge-optimized |
| **Model Server** | llama.cpp | C/C++ performance; Metal support; OpenAI-compatible |
| **Apple Silicon** | Metal Performance Shaders | 3-5x speedup; automatic in llama.cpp |
| **Quantization** | Q4_K | Optimal speed/quality tradeoff |
| **Deployment** | Both local + cloud fallback | Privacy + reliability |

**Total App Bundle Size**: ~500 MB (with Whisper model)
**Dynamic Download** (on first local LLM run): ~2.5 GB (Phi-3 Mini)
**Memory Requirement**: 8 GB RAM minimum, 16 GB+ recommended

---

**Report Generated**: November 16, 2025
**Status**: Ready for Implementation
**Next Step**: Begin Phase 1 integration of llama.cpp server launcher
